{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pathlib import Path\n",
    "from typing import Any"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-clean data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data for train data\n",
    "airport = 'KATL'\n",
    "train_label = f'data/prescreened_train_labels/prescreened_train_labels_{airport}.csv.bz2'\n",
    "pushback = pd.read_csv(train_label, compression='bz2', parse_dates=['timestamp'])\n",
    "etd_label = f'data/{airport}/{airport}_etd.csv.bz2'\n",
    "etd = pd.read_csv(etd_label, compression='bz2', parse_dates=['timestamp', 'departure_runway_estimated_time'])\n",
    "lamp_label = f'data/{airport}/{airport}_lamp.csv.bz2'\n",
    "lamp = pd.read_csv(lamp_label, compression='bz2', parse_dates=['timestamp', 'forecast_timestamp'])\n",
    "mfs_label = f'data/{airport}/{airport}_mfs.csv.bz2'\n",
    "mfs = pd.read_csv(mfs_label, compression='bz2')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Function\n",
    "def clean_data(pushback, etd, lamp, mfs):\n",
    "    #Take out unwanted columns\n",
    "    lamp = lamp.drop('wind_direction', axis=1)\n",
    "    mfs = mfs[mfs['isdeparture'] == True]\n",
    "    #Make new timestamps and forecast timestamps to prep for merges\n",
    "    lamp['timestamp_plus_30'] = lamp['timestamp'] + pd.Timedelta(minutes=30)\n",
    "    lamp['timestamp_plus_45'] = lamp['timestamp'] + pd.Timedelta(minutes=45)\n",
    "    lamp['timestamp_plus_60'] = lamp['timestamp'] + pd.Timedelta(minutes=60)\n",
    "    lamp['timestamp_plus_75'] = lamp['timestamp'] + pd.Timedelta(minutes=75)\n",
    "    lamp['forecast_plus_15'] = lamp['forecast_timestamp'] + pd.Timedelta(minutes=15)\n",
    "    lamp['forecast_plus_30'] = lamp['forecast_timestamp'] + pd.Timedelta(minutes=30)\n",
    "    lamp['forecast_plus_45'] = lamp['forecast_timestamp'] + pd.Timedelta(minutes=45)\n",
    "    # Drop timestamp column\n",
    "    lamp = lamp.drop('timestamp', axis=1)\n",
    "    #Filter timestamps in lamp that I want to merge with pushback\n",
    "    lamp1 = lamp[lamp['timestamp_plus_30'] == lamp['forecast_timestamp']]\n",
    "    lamp2 = lamp[lamp['timestamp_plus_45'] == lamp['forecast_plus_15']]\n",
    "    lamp3 = lamp[lamp['timestamp_plus_60'] == lamp['forecast_plus_30']]\n",
    "    lamp4 = lamp[lamp['timestamp_plus_75'] == lamp['forecast_plus_45']]\n",
    "\n",
    "    # merge the dataframes based on multiple conditions\n",
    "    # Inner Merge, note that this inner merge exlucdes all values in pushback where there is no lamp data \n",
    "    merge1 = pd.merge(pushback, lamp1, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_30'])\n",
    "    merge2 = pd.merge(pushback, lamp2, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_45'])\n",
    "    merge3 = pd.merge(pushback, lamp3, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_60'])\n",
    "    merge4 = pd.merge(pushback, lamp4, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_75'])\n",
    "    #Concat \n",
    "    merged_df = pd.concat([merge1, merge2, merge3, merge4], ignore_index=True)\n",
    "    #Get the rows from pushback with no lamp data (lamp data is filled with nan)\n",
    "        #Take the columns from the merged_df that I want to merge with pushback\n",
    "    temp1 = merged_df[['gufi', 'timestamp', 'minutes_until_pushback']]\n",
    "        #Outer merge pushback with temp1 so I can see which rows are in pushback but not in merged_df\n",
    "    temp2 = pd.merge(pushback, temp1, how='outer', on=['gufi', 'timestamp', 'minutes_until_pushback'], indicator=True)\n",
    "        #Get only the rows where lamp data is not available\n",
    "    result_df = temp2[temp2['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "        #Now I have all original rows from the original pushback, and if lamp data is not available, those are filled with nan\n",
    "    pushback = pd.merge(merged_df, result_df, on=['gufi', 'timestamp', 'airport', 'minutes_until_pushback'], how='outer', indicator=True).drop('_merge', axis=1)\n",
    "        #Remove columns I added for merging purposes\n",
    "    pushback = pushback[['gufi', 'timestamp', 'airport', 'minutes_until_pushback', 'forecast_timestamp', 'temperature', 'wind_speed', 'wind_gust', 'cloud_ceiling', 'visibility', 'cloud', 'lightning_prob', 'precip']]\n",
    "\n",
    "\n",
    "    #Merge mfs onto pushback\n",
    "        #drop unnecessary column    \n",
    "    mfs = mfs.drop('isdeparture', axis=1)\n",
    "        #Merge\n",
    "    pushback = pd.merge(pushback, mfs, how='left', left_on=['gufi'], right_on=['gufi'])\n",
    "\n",
    "    #Merges etd ont pushback\n",
    "    #Rounds timestamp to nearest 15 minutes\n",
    "    etd['rounded_timestamp'] = etd['timestamp'].dt.round('15min')\n",
    "    #I want all values to round up so if rounded_timestamp is less than timestamp, it adds 15 minutes\n",
    "    etd.loc[etd['timestamp'] > etd['rounded_timestamp'], 'rounded_timestamp'] += pd.Timedelta(minutes=15)\n",
    "    # Drop Duplicates and keep more recent predicted time \n",
    "        # Sort etd by timestamp in descending order (note that drop_duplicates keeps first occurence)\n",
    "    etd = etd.sort_values('timestamp', ascending=False)\n",
    "        # Drop duplicates\n",
    "    etd.drop_duplicates(inplace=True, subset=['gufi', 'rounded_timestamp'])\n",
    "    #Drop timestamp column \n",
    "    etd = etd.drop('timestamp', axis=1)\n",
    "    etd = etd.reset_index(drop=True)\n",
    "    #Left Merge pushback and etd on gufi\n",
    "    pushback = pd.merge(pushback, etd, on='gufi', how='left')\n",
    "    #Take out all observations that rounded_timestamp occurs after timestamp\n",
    "    pushback = pushback[pushback['timestamp'] >= pushback['rounded_timestamp']]\n",
    "    #Sort values and drop duplicates\n",
    "    pushback = pushback.sort_values('rounded_timestamp', ascending=False)\n",
    "    pushback.drop_duplicates(inplace=True, subset=['gufi', 'timestamp', 'minutes_until_pushback'])\n",
    "    #Drop rounded_timestamp\n",
    "    pushback = pushback.drop('rounded_timestamp', axis=1)\n",
    "    pushback = pushback.sort_values(['gufi', 'timestamp'])\n",
    "    pushback = pushback.reset_index(drop=True)\n",
    "\n",
    "    # Pushback is typically about 15 minutes before departure so\n",
    "        #Estimate the time pushback occurs\n",
    "    pushback['benchmark_pushback_estimated_time'] = pushback['departure_runway_estimated_time'] - pd.Timedelta(minutes=15)\n",
    "        #Create a benchmark time\n",
    "    pushback['benchmark_pushback'] = (pushback['benchmark_pushback_estimated_time'] - pushback['timestamp']) / pd.Timedelta(minutes=1)\n",
    "\n",
    "    # Create new variables for day of week, month, hour, and year\n",
    "    pushback['day_of_week'] = pushback['departure_runway_estimated_time'].dt.day_name()\n",
    "    pushback['month'] = pushback['departure_runway_estimated_time'].dt.strftime('%B')\n",
    "    pushback['hour'] = pushback['departure_runway_estimated_time'].dt.hour\n",
    "    pushback['year'] = pushback['departure_runway_estimated_time'].dt.year\n",
    "\n",
    "    #Fill categorical NA \n",
    "    pushback['aircraft_type'].fillna('OTHER', inplace=True)\n",
    "    pushback['flight_type'].fillna('OTHER', inplace=True)\n",
    "    pushback['major_carrier'].fillna('OTHER', inplace=True)\n",
    "\n",
    "    #drop uneccessary variables\n",
    "    drop_cols = ['timestamp', 'airport', 'forecast_timestamp', 'departure_runway_estimated_time', 'benchmark_pushback_estimated_time']\n",
    "    pushback = pushback.drop(drop_cols, axis=1)\n",
    "    pushback = pushback.reset_index(drop=True)\n",
    "\n",
    "    return pushback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot (pushback):\n",
    "    \n",
    "    #One-hot encoding\n",
    "    categorical_cols = ['cloud', 'lightning_prob', 'precip', 'day_of_week', 'month', 'aircraft_engine_class', 'flight_type', 'aircraft_type', 'major_carrier', 'year']\n",
    "    #Try\n",
    "    pushback_onehot = pushback\n",
    "    for col in categorical_cols:\n",
    "        pushback_onehot[col] = pushback_onehot[col].astype('category')\n",
    "    #Drop Rows with NaN lamp data\n",
    "    pushback_onehot = pushback_onehot.dropna()\n",
    "    return pushback_onehot\n",
    "    \n",
    "    #One-hot encoding for lasso\n",
    "    #categorical_cols = ['cloud', 'lightning_prob', 'precip', 'day_of_week', 'month', 'aircraft_engine_class', 'flight_type', 'aircraft_type']\n",
    "    #encoded_cols = pd.get_dummies(pushback[categorical_cols], prefix=categorical_cols, drop_first=True)\n",
    "    #pushback_onehot = pd.concat([pushback, encoded_cols], axis=1)\n",
    "    #pushback_onehot = pushback_onehot.drop(categorical_cols, axis=1)\n",
    "\n",
    "    #Drop Rows with NaN lamp data\n",
    "    #pushback_onehot = pushback_onehot.dropna()\n",
    "\n",
    "    #return pushback_onehot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply functions to data\n",
    "\n",
    "#Clean Data\n",
    "pushback = clean_data(pushback, etd, lamp, mfs)\n",
    "#One-hot encode data\n",
    "pushback_onehot = onehot(pushback)\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "benchmark = mean_absolute_error(pushback['minutes_until_pushback'], pushback['benchmark_pushback'])\n",
    "#%%\n",
    "\n",
    "#%% assign parameters\n",
    "params = {\n",
    "    'boosting_type': 'goss',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'max_bins' : 1000,\n",
    "    'verbose': 10,\n",
    "    'min_data_in_leaf' : 250,\n",
    "    'n_jobs': -1,\n",
    "    'learning_rate': 0.053555531051409686,\n",
    "    'max_depth': 17,\n",
    "    'num_leaves': 800,\n",
    "    'reg_alpha' : 0.9752885124310164,\n",
    "    'reg_lambda' : 0.9845786100240768,\n",
    "    'feature_fraction' : 0.9932740303336315,\n",
    "    'feature_pre_filter' : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pushback_onehot.drop('minutes_until_pushback', axis=1)\n",
    "X = X.drop('gufi', axis=1)\n",
    "y = pushback_onehot['minutes_until_pushback']\n",
    "\n",
    "gbm_model_final = lgb.train(params,\n",
    "                lgb.Dataset(X, y),\n",
    "                num_boost_round = 1000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data for test data\n",
    "submission_format = f'data/code_execution_development_data/test_labels.csv'\n",
    "pushback_predict = pd.read_csv(submission_format, parse_dates=['timestamp'])\n",
    "pushback_predict = pushback_predict[pushback_predict['airport'] == f\"{airport}\"]\n",
    "\n",
    "etd_label = f'data/code_execution_development_data/{airport}/{airport}_etd.csv.bz2'\n",
    "etd = pd.read_csv(etd_label, compression='bz2', parse_dates=['timestamp', 'departure_runway_estimated_time'])\n",
    "\n",
    "lamp_label = f'data/code_execution_development_data/{airport}/{airport}_lamp.csv.bz2'\n",
    "lamp = pd.read_csv(lamp_label, compression='bz2', parse_dates=['timestamp', 'forecast_timestamp'])\n",
    "\n",
    "mfs_label = f'data/code_execution_development_data/{airport}/{airport}_mfs.csv.bz2'\n",
    "mfs = pd.read_csv(mfs_label, compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Test Data\n",
    "pushback_predict = clean_data(pushback_predict, etd, lamp, mfs)\n",
    "pushback = pushback.assign(test=0)\n",
    "pushback_predict = pushback_predict.assign(test=1)\n",
    "pushback_combined = pd.concat([pushback, pushback_predict])\n",
    "pushback_combined = onehot(pushback_combined)\n",
    "pushback_predict_onehot = pushback_combined[pushback_combined['test']==1]\n",
    "pushback_predict_onehot = pushback_predict_onehot.drop('test', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features and target variables\n",
    "X = pushback_predict_onehot.drop('minutes_until_pushback', axis=1)\n",
    "X_test = X.drop('gufi', axis=1)\n",
    "y = pushback_predict_onehot['minutes_until_pushback']\n",
    "\n",
    "#Predict\n",
    "y_pred_gbm = gbm_model_final.predict(X_test, num_iteration=gbm_model_final.best_iteration)\n",
    "model_mae = mean_absolute_error(y, y_pred_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = mean_absolute_error(pushback['minutes_until_pushback'], pushback['benchmark_pushback'])\n",
    "\n",
    "print(\"Benchmark MAE\", benchmark)\n",
    "print(\"Model MAE:\",  model_mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Models to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model_final.save_model(f\"{airport}_model.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me397_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
