{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import process_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data\n",
    "airport = 'KATL'\n",
    "\n",
    "train_label = f'data/prescreened_train_labels/prescreened_train_labels_{airport}.csv.bz2'\n",
    "pushback = pd.read_csv(train_label, compression='bz2', parse_dates=['timestamp'])\n",
    "\n",
    "etd_label = f'data/{airport}/{airport}_etd.csv.bz2'\n",
    "etd = pd.read_csv(etd_label, compression='bz2', parse_dates=['timestamp', 'departure_runway_estimated_time'])\n",
    "\n",
    "lamp_label = f'data/{airport}/{airport}_lamp.csv.bz2'\n",
    "lamp = pd.read_csv(lamp_label, compression='bz2', parse_dates=['timestamp', 'forecast_timestamp'])\n",
    "\n",
    "mfs_label = f'data/{airport}/{airport}_mfs.csv.bz2'\n",
    "mfs = pd.read_csv(mfs_label, compression='bz2')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out unwanted columns\n",
    "lamp = lamp.drop('wind_direction', axis=1)\n",
    "mfs = mfs[mfs['isdeparture'] == True]\n",
    "\n",
    "#Make new timestamps and forecast timestamps to prep for merges\n",
    "lamp['timestamp_plus_30'] = lamp['timestamp'] + pd.Timedelta(minutes=30)\n",
    "lamp['timestamp_plus_45'] = lamp['timestamp'] + pd.Timedelta(minutes=45)\n",
    "lamp['timestamp_plus_60'] = lamp['timestamp'] + pd.Timedelta(minutes=60)\n",
    "lamp['timestamp_plus_75'] = lamp['timestamp'] + pd.Timedelta(minutes=75)\n",
    "\n",
    "lamp['forecast_plus_15'] = lamp['forecast_timestamp'] + pd.Timedelta(minutes=15)\n",
    "lamp['forecast_plus_30'] = lamp['forecast_timestamp'] + pd.Timedelta(minutes=30)\n",
    "lamp['forecast_plus_45'] = lamp['forecast_timestamp'] + pd.Timedelta(minutes=45)\n",
    "\n",
    "#Drop timestamp column\n",
    "lamp = lamp.drop('timestamp', axis=1)\n",
    "\n",
    "#Filter timestamps in lamp that I want to merge with pushback\n",
    "lamp1 = lamp[lamp['timestamp_plus_30'] == lamp['forecast_timestamp']]\n",
    "lamp2 = lamp[lamp['timestamp_plus_45'] == lamp['forecast_plus_15']]\n",
    "lamp3 = lamp[lamp['timestamp_plus_60'] == lamp['forecast_plus_30']]\n",
    "lamp4 = lamp[lamp['timestamp_plus_75'] == lamp['forecast_plus_45']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes based on multiple conditions\n",
    "\n",
    "# Inner Merge, note that this inner merge exlucdes all values in pushback where there is no lamp data \n",
    "merge1 = pd.merge(pushback, lamp1, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_30'])\n",
    "merge2 = pd.merge(pushback, lamp2, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_45'])\n",
    "merge3 = pd.merge(pushback, lamp3, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_60'])\n",
    "merge4 = pd.merge(pushback, lamp4, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_75'])\n",
    "\n",
    "merged_df = pd.concat([merge1, merge2, merge3, merge4], ignore_index=True)\n",
    "\n",
    "#Get the rows from pushback with no lamp data (lamp data is filled with nan)\n",
    "    #Take the columns from the merged_df that I want to merge with pushback\n",
    "temp1 = merged_df[['gufi', 'timestamp', 'minutes_until_pushback']]\n",
    "    #Outer merge pushback with temp1 so I can see which rows are in pushback but not in merged_df\n",
    "temp2 = pd.merge(pushback, temp1, how='outer', on=['gufi', 'timestamp', 'minutes_until_pushback'], indicator=True)\n",
    "    #Get only the rows where lamp data is not available\n",
    "result_df = temp2[temp2['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "    #Now I have all original rows from the original pushback, and if lamp data is not available, those are filled with nan\n",
    "pushback = pd.merge(merged_df, result_df, on=['gufi', 'timestamp', 'airport', 'minutes_until_pushback'], how='outer', indicator=True).drop('_merge', axis=1)\n",
    "    #Remove columns I added for merging purposes\n",
    "pushback = pushback.iloc[:, :13]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge mfs onto pushback\n",
    "    #drop unnecessary column    \n",
    "mfs = mfs.drop('isdeparture', axis=1)\n",
    "    #Merge\n",
    "pushback = pd.merge(pushback, mfs, how='left', left_on=['gufi'], right_on=['gufi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rounds timestamp to nearest 15 minutes\n",
    "etd['rounded_timestamp'] = etd['timestamp'].dt.round('15min')\n",
    "\n",
    "#I want all values to round up so if rounded_timestamp is less than timestamp, it adds 15 minutes\n",
    "etd.loc[etd['timestamp'] > etd['rounded_timestamp'], 'rounded_timestamp'] += pd.Timedelta(minutes=15)\n",
    "\n",
    "# Drop Duplicates and keep more recent predicted time \n",
    "    # Sort etd by timestamp in descending order (note that drop_duplicates keeps first occurence)\n",
    "etd = etd.sort_values('timestamp', ascending=False)\n",
    "    # Drop duplicates\n",
    "etd.drop_duplicates(inplace=True, subset=['gufi', 'rounded_timestamp'])\n",
    "\n",
    "#Drop timestamp column \n",
    "etd = etd.drop('timestamp', axis=1)\n",
    "etd = etd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge etd onto pushback\n",
    "    #Left Merge pushback and etd on gufi\n",
    "pushback = pd.merge(pushback, etd, on='gufi', how='left')\n",
    "    #Take out all observations that rounded_timestamp occurs after timestamp\n",
    "pushback = pushback[pushback['timestamp'] >= pushback['rounded_timestamp']]\n",
    "    #Sort values and drop duplicates\n",
    "pushback = pushback.sort_values('rounded_timestamp', ascending=False)\n",
    "pushback.drop_duplicates(inplace=True, subset=['gufi', 'timestamp', 'minutes_until_pushback'])\n",
    "    #Drop rounded_timestamp\n",
    "pushback = pushback.drop('rounded_timestamp', axis=1)\n",
    "pushback = pushback.sort_values(['gufi', 'timestamp'])\n",
    "pushback = pushback.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data / Create Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushback is typically about 15 minutes before departure so\n",
    "    #Estimate the time pushback occurs\n",
    "pushback['benchmark_pushback_estimated_time'] = pushback['departure_runway_estimated_time'] - pd.Timedelta(minutes=15)\n",
    "    #Create a benchmark time\n",
    "pushback['benchmark_pushback'] = (pushback['benchmark_pushback_estimated_time'] - pushback['timestamp']) / pd.Timedelta(minutes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables for day of week and month\n",
    "pushback['day_of_week'] = pushback['departure_runway_estimated_time'].dt.day_name()\n",
    "pushback['month'] = pushback['departure_runway_estimated_time'].dt.strftime('%B')\n",
    "pushback['hour'] = pushback['departure_runway_estimated_time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill categorical NA \n",
    "pushback['aircraft_type'].fillna('OTHER', inplace=True)\n",
    "pushback['flight_type'].fillna('OTHER', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop uneccessary variables\n",
    "drop_cols = ['timestamp', 'airport', 'forecast_timestamp', 'major_carrier', 'departure_runway_estimated_time', 'benchmark_pushback_estimated_time']\n",
    "pushback = pushback.drop(drop_cols, axis=1)\n",
    "pushback = pushback.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding\n",
    "categorical_cols = ['cloud', 'lightning_prob', 'precip', 'day_of_week', 'month', 'aircraft_engine_class', 'flight_type', 'aircraft_type', 'major_carrier', 'year']\n",
    "\n",
    "pushback_onehot = pushback\n",
    "for col in categorical_cols:\n",
    "    pushback_onehot[col] = pushback_onehot[col].astype('category')\n",
    "\n",
    "#Drop Rows with NaN lamp data\n",
    "pushback_onehot = pushback_onehot.dropna()\n",
    "\n",
    "#One-hot encoding\n",
    "#categorical_cols = ['cloud', 'lightning_prob', 'precip', 'day_of_week', 'month', 'aircraft_engine_class', 'flight_type', 'aircraft_type']\n",
    "#encoded_cols = pd.get_dummies(pushback[categorical_cols], prefix=categorical_cols, drop_first=True)\n",
    "#pushback_onehot = pd.concat([pushback, encoded_cols], axis=1)\n",
    "#pushback_onehot = pushback_onehot.drop(categorical_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Rows with NaN lamp data\n",
    "pushback_onehot = pushback_onehot.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test Split\n",
    "# define the features and target variables\n",
    "X = pushback_onehot.drop(['minutes_until_pushback', 'gufi'], axis=1)\n",
    "y = pushback_onehot['minutes_until_pushback']\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(pushback['minutes_until_pushback'], pushback['benchmark_pushback'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, X_test['benchmark_pushback'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test data for gbm model\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space\n",
    "depth_start = 10\n",
    "\n",
    "space = {\n",
    "    'num_leaves': hp.quniform('num_leaves', 400, 1000, 25),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -4, 0),\n",
    "    'max_depth': hp.choice('max_depth', range(depth_start, 21)),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.5, 1)\n",
    "}\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    # Convert integer parameters to integer values\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_jobs'] = -1\n",
    "    params['min_data_in_leaf'] = 250\n",
    "    params['boosting'] = 'goss'\n",
    "    params['feature_pre_filter'] = False\n",
    "    \n",
    "    \n",
    "    # Train the LightGBM model with the specified hyperparameters\n",
    "    model = lgb.train(params, train_data, valid_sets=test_data, num_boost_round=1000, early_stopping_rounds=10) #, verbose_eval=False)\n",
    "    \n",
    "    # Predict on the validation set and compute the mean squared error\n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    \n",
    "    # Return the loss to be minimized\n",
    "    return {'loss': mae, 'status': STATUS_OK}\n",
    "\n",
    "# Define the optimization algorithm\n",
    "tpe_algo = tpe.suggest\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(objective, space, algo=tpe_algo, max_evals=25)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:', best)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_params = {\n",
    "    'num_leaves': int(best['num_leaves']),\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'max_depth': int(best['max_depth']) + depth_start,\n",
    "    'reg_alpha': best['reg_alpha'],\n",
    "    'reg_lambda': best['reg_lambda'],\n",
    "    'feature_fraction' : best['feature_fraction'],\n",
    "    'min_data_in_leaf': 200,\n",
    "    'n_jobs': -1,\n",
    "    'boosting' : 'goss',\n",
    "    'feature_pre_filter' : False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model\n",
    "\n",
    "final_model = lgb.train(best_params, train_data, valid_sets=test_data, num_boost_round=1000, early_stopping_rounds=10, verbose_eval=True)\n",
    "train_predictions = final_model.predict(X_train)\n",
    "train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "final_predictions = final_model.predict(X_test)\n",
    "final_mae = mean_absolute_error(y_test, final_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline1: \", baseline_mae)\n",
    "print(\"Baseline2\", baseline_mae2)\n",
    "print(\"Train mae:\", train_mae)\n",
    "print(\"Final mae:\", final_mae)\n",
    "print(best['max_depth'] + depth_start)\n",
    "print(best)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Lasso regression model to the training data\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target variable on the test data\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "y_pred_lasso = np.where(y_pred_lasso > y_train.max(), y_train.max(), y_pred_lasso)\n",
    "y_pred_lasso = np.where(y_pred_lasso < y_train.min(), y_train.min(), y_pred_lasso)\n",
    "\n",
    "#Calculate MAE for Lasso\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
    "print(mae_lasso)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit random forest\n",
    "rf = RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=1994, max_depth=15)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions and mae\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "print(mae_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me397_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
