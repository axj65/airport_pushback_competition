{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(pushback, etd, lamp, mfs):\n",
    "    #Take out unwanted columns\n",
    "    lamp = lamp.drop('wind_direction', axis=1)\n",
    "    mfs = mfs[mfs['isdeparture'] == True]\n",
    "    #Make new timestamps and forecast timestamps to prep for merges\n",
    "    lamp['timestamp_plus_30'] = lamp['timestamp'] + pd.Timedelta(minutes=30)\n",
    "    lamp['timestamp_plus_45'] = lamp['timestamp'] + pd.Timedelta(minutes=45)\n",
    "    lamp['timestamp_plus_60'] = lamp['timestamp'] + pd.Timedelta(minutes=60)\n",
    "    lamp['timestamp_plus_75'] = lamp['timestamp'] + pd.Timedelta(minutes=75)\n",
    "    lamp['forecast_plus_15'] = lamp['forecast_timestamp'] + pd.Timedelta(minutes=15)\n",
    "    lamp['forecast_plus_30'] = lamp['forecast_timestamp'] + pd.Timedelta(minutes=30)\n",
    "    lamp['forecast_plus_45'] = lamp['forecast_timestamp'] + pd.Timedelta(minutes=45)\n",
    "    # Drop timestamp column\n",
    "    lamp = lamp.drop('timestamp', axis=1)\n",
    "    #Filter timestamps in lamp that I want to merge with pushback\n",
    "    lamp1 = lamp[lamp['timestamp_plus_30'] == lamp['forecast_timestamp']]\n",
    "    lamp2 = lamp[lamp['timestamp_plus_45'] == lamp['forecast_plus_15']]\n",
    "    lamp3 = lamp[lamp['timestamp_plus_60'] == lamp['forecast_plus_30']]\n",
    "    lamp4 = lamp[lamp['timestamp_plus_75'] == lamp['forecast_plus_45']]\n",
    "\n",
    "    # merge the dataframes based on multiple conditions\n",
    "    # Inner Merge, note that this inner merge exlucdes all values in pushback where there is no lamp data \n",
    "    merge1 = pd.merge(pushback, lamp1, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_30'])\n",
    "    merge2 = pd.merge(pushback, lamp2, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_45'])\n",
    "    merge3 = pd.merge(pushback, lamp3, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_60'])\n",
    "    merge4 = pd.merge(pushback, lamp4, how='inner', left_on=['timestamp'], right_on=['timestamp_plus_75'])\n",
    "    #Concat \n",
    "    merged_df = pd.concat([merge1, merge2, merge3, merge4], ignore_index=True)\n",
    "    #Get the rows from pushback with no lamp data (lamp data is filled with nan)\n",
    "        #Take the columns from the merged_df that I want to merge with pushback\n",
    "    temp1 = merged_df[['gufi', 'timestamp', 'minutes_until_pushback']]\n",
    "        #Outer merge pushback with temp1 so I can see which rows are in pushback but not in merged_df\n",
    "    temp2 = pd.merge(pushback, temp1, how='outer', on=['gufi', 'timestamp', 'minutes_until_pushback'], indicator=True)\n",
    "        #Get only the rows where lamp data is not available\n",
    "    result_df = temp2[temp2['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "        #Now I have all original rows from the original pushback, and if lamp data is not available, those are filled with nan\n",
    "    pushback = pd.merge(merged_df, result_df, on=['gufi', 'timestamp', 'airport', 'minutes_until_pushback'], how='outer', indicator=True).drop('_merge', axis=1)\n",
    "        #Remove columns I added for merging purposes\n",
    "    pushback = pushback[['gufi', 'timestamp', 'airport', 'minutes_until_pushback', 'forecast_timestamp', 'temperature', 'wind_speed', 'wind_gust', 'cloud_ceiling', 'visibility', 'cloud', 'lightning_prob', 'precip']]\n",
    "\n",
    "\n",
    "    #Merge mfs onto pushback\n",
    "        #drop unnecessary column    \n",
    "    mfs = mfs.drop('isdeparture', axis=1)\n",
    "        #Merge\n",
    "    pushback = pd.merge(pushback, mfs, how='left', left_on=['gufi'], right_on=['gufi'])\n",
    "\n",
    "    #Merges etd ont pushback\n",
    "    #Rounds timestamp to nearest 15 minutes\n",
    "    etd['rounded_timestamp'] = etd['timestamp'].dt.round('15min')\n",
    "    #I want all values to round up so if rounded_timestamp is less than timestamp, it adds 15 minutes\n",
    "    etd.loc[etd['timestamp'] > etd['rounded_timestamp'], 'rounded_timestamp'] += pd.Timedelta(minutes=15)\n",
    "    # Drop Duplicates and keep more recent predicted time \n",
    "        # Sort etd by timestamp in descending order (note that drop_duplicates keeps first occurence)\n",
    "    etd = etd.sort_values('timestamp', ascending=False)\n",
    "        # Drop duplicates\n",
    "    etd.drop_duplicates(inplace=True, subset=['gufi', 'rounded_timestamp'])\n",
    "    #Drop timestamp column \n",
    "    etd = etd.drop('timestamp', axis=1)\n",
    "    etd = etd.reset_index(drop=True)\n",
    "    #Left Merge pushback and etd on gufi\n",
    "    pushback = pd.merge(pushback, etd, on='gufi', how='left')\n",
    "    #Take out all observations that rounded_timestamp occurs after timestamp\n",
    "    pushback = pushback[pushback['timestamp'] >= pushback['rounded_timestamp']]\n",
    "    #Sort values and drop duplicates\n",
    "    pushback = pushback.sort_values('rounded_timestamp', ascending=False)\n",
    "    pushback.drop_duplicates(inplace=True, subset=['gufi', 'timestamp', 'minutes_until_pushback'])\n",
    "    #Drop rounded_timestamp\n",
    "    pushback = pushback.drop('rounded_timestamp', axis=1)\n",
    "    pushback = pushback.sort_values(['gufi', 'timestamp'])\n",
    "    pushback = pushback.reset_index(drop=True)\n",
    "\n",
    "    # Pushback is typically about 15 minutes before departure so\n",
    "        #Estimate the time pushback occurs\n",
    "    pushback['benchmark_pushback_estimated_time'] = pushback['departure_runway_estimated_time'] - pd.Timedelta(minutes=15)\n",
    "        #Create a benchmark time\n",
    "    pushback['benchmark_pushback'] = (pushback['benchmark_pushback_estimated_time'] - pushback['timestamp']) / pd.Timedelta(minutes=1)\n",
    "\n",
    "    # Create new variables for day of week and month and year\n",
    "    pushback['day_of_week'] = pushback['departure_runway_estimated_time'].dt.day_name()\n",
    "    pushback['month'] = pushback['departure_runway_estimated_time'].dt.strftime('%B')\n",
    "    pushback['hour'] = pushback['departure_runway_estimated_time'].dt.hour\n",
    "    pushback['year'] = pushback['departure_runway_estimated_time'].dt.year\n",
    "\n",
    "    #Fill categorical NA \n",
    "    pushback['aircraft_type'].fillna('OTHER', inplace=True)\n",
    "    pushback['flight_type'].fillna('OTHER', inplace=True)\n",
    "    pushback['major_carrier'].fillna('OTHER', inplace=True)\n",
    "\n",
    "    #drop uneccessary variables\n",
    "    drop_cols = ['timestamp', 'airport', 'forecast_timestamp', 'departure_runway_estimated_time', 'benchmark_pushback_estimated_time']\n",
    "    pushback = pushback.drop(drop_cols, axis=1)\n",
    "    pushback = pushback.reset_index(drop=True)\n",
    "\n",
    "    return pushback\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot (pushback):\n",
    "    #Drop Airport\n",
    "    pushback = pushback.drop('airport', axis=1)\n",
    "    \n",
    "    #One-hot encoding\n",
    "    categorical_cols = ['cloud', 'lightning_prob', 'precip', 'day_of_week', 'month', 'aircraft_engine_class', 'flight_type', 'aircraft_type', 'major_carrier', 'year']\n",
    "\n",
    "    pushback_onehot = pushback\n",
    "    for col in categorical_cols:\n",
    "        pushback_onehot[col] = pushback_onehot[col].astype('category')\n",
    "\n",
    "    return pushback_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(solution_directory: Path, airport_code: str) -> Any:\n",
    "    \"\"\"Load a specific model asset from disk.\"\"\"\n",
    "    model_filename = f\"models/{airport_code}_model.txt\"\n",
    "    model = lgb.Booster(model_file=str(solution_directory / model_filename))\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "solution_directory = Path(\".\")  # Set the solution directory to the current directory\n",
    "\n",
    "airport_codes = ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', 'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA', 'KATL_na', 'KCLT_na', 'KDEN_na', 'KDFW_na', 'KJFK_na', 'KMEM_na', 'KMIA_na', 'KORD_na', 'KPHX_na', 'KSEA_na']\n",
    "\n",
    "models = {}\n",
    "for airport_code in airport_codes:\n",
    "    models[airport_code] = load_model(solution_directory, airport_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KATL\n",
      "0    AAL1008.ATL.DFW.210607.2033.0110.TFM\n",
      "Name: gufi, dtype: object\n",
      "KCLT\n",
      "0    AAL1005.CLT.LAS.210826.1259.0084.TFM\n",
      "Name: gufi, dtype: object\n",
      "KDEN\n",
      "0    AAL1046.DEN.JAC.210205.2011.0001.TFM_TFDM\n",
      "Name: gufi, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/vxz07v2s2pdb7dc0707cwt1c0000gn/T/ipykernel_74716/647321355.py:27: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mfs = pd.read_csv(mfs_label, compression='bz2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KDFW\n",
      "0    AAL1001.DFW.CUN.210201.1626.0083.TFM\n",
      "Name: gufi, dtype: object\n",
      "KJFK\n",
      "0    AAL1.JFK.LAX.210202.1357.0089.TFM\n",
      "Name: gufi, dtype: object\n",
      "KMEM\n",
      "0    AAL1148.MEM.DFW.210606.0128.0003.TFM_TFDM\n",
      "Name: gufi, dtype: object\n",
      "KMIA\n",
      "0    AAL1007.MIA.POP.210210.1746.0072.TFM\n",
      "Name: gufi, dtype: object\n",
      "KORD\n",
      "0    AAL1002.ORD.DCA.210201.1457.0077.TFM\n",
      "Name: gufi, dtype: object\n",
      "KPHX\n",
      "0    AAL1078.PHX.LAX.210826.1304.0054.TFM\n",
      "Name: gufi, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/vxz07v2s2pdb7dc0707cwt1c0000gn/T/ipykernel_74716/647321355.py:27: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mfs = pd.read_csv(mfs_label, compression='bz2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSEA\n",
      "0    AAL1006.SEA.DFW.210827.1300.0052.TFM_TFDM\n",
      "Name: gufi, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Append airport test data for all airports\n",
    "\n",
    "#Create empty dataframe to append airport test data\n",
    "combined_pushback = pd.DataFrame()\n",
    "\n",
    "airports = ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', 'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA']\n",
    "\n",
    "for i in airports:\n",
    "    #Read in data for train data\n",
    "    airport = i\n",
    "\n",
    "    submission_format = f'data/submission_format.csv'\n",
    "    #submission_format = f'data/code_execution_development_data/submission_format.csv'\n",
    "    pushback_predict = pd.read_csv(submission_format, parse_dates=['timestamp'])\n",
    "    pushback_predict = pushback_predict[pushback_predict['airport'] == f\"{airport}\"]\n",
    "\n",
    "    etd_label = f'data/{airport}/{airport}_etd.csv.bz2'\n",
    "    #etd_label = f'data/code_execution_development_data/{airport}/{airport}_etd.csv.bz2'\n",
    "    etd = pd.read_csv(etd_label, compression='bz2', parse_dates=['timestamp', 'departure_runway_estimated_time'])\n",
    "\n",
    "    lamp_label = f'data/{airport}/{airport}_lamp.csv.bz2'\n",
    "    #lamp_label = f'data/code_execution_development_data/{airport}/{airport}_lamp.csv.bz2'\n",
    "    lamp = pd.read_csv(lamp_label, compression='bz2', parse_dates=['timestamp', 'forecast_timestamp'])\n",
    "\n",
    "    mfs_label = f'data/{airport}/{airport}_mfs.csv.bz2'\n",
    "    #mfs_label = f'data/code_execution_development_data/{airport}/{airport}_mfs.csv.bz2'\n",
    "    mfs = pd.read_csv(mfs_label, compression='bz2')\n",
    "\n",
    "    #Clean Data\n",
    "    pushback_predict = clean_data(pushback_predict, etd, lamp, mfs)\n",
    "    pushback_predict['airport'] = airport\n",
    "    print(airport)\n",
    "    print(pushback_predict['gufi'].head(1))\n",
    "    combined_pushback = pd.concat([combined_pushback, pushback_predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the subsets of data\n",
    "airport_data = {}\n",
    "\n",
    "# Loop over the airports and create subsets of data for each airport\n",
    "for airport in airports:\n",
    "    # Create a subset of data for the current airport\n",
    "    airport_subset = combined_pushback[combined_pushback['airport'] == airport]\n",
    "\n",
    "    # Apply one-hot encoding to subset\n",
    "    airport_test = onehot(airport_subset)\n",
    "\n",
    "    #Create response and predictor variables\n",
    "    X = airport_test.drop('minutes_until_pushback', axis=1)\n",
    "    X = X.drop('gufi', axis=1)\n",
    "    y = airport_test['minutes_until_pushback']\n",
    "\n",
    "    #Drop columns with Nan data\n",
    "    X_na = X.dropna(axis=1)\n",
    "\n",
    "    #Make prediction for normal data\n",
    "    y_pred = models[airport].predict(X).round().astype(int)\n",
    "\n",
    "    #Make prediction for data with weather NaNs\n",
    "    y_pred_na = models[airport + \"_na\"].predict(X_na).round().astype(int)\n",
    "\n",
    "    #Change predictions to NaN predictions if weather is Nan\n",
    "    nan_rows = X.isna().any(axis=1).to_numpy()\n",
    "    for i, has_nan in enumerate(nan_rows):\n",
    "        if has_nan:\n",
    "            y_pred[i] = y_pred_na[i]\n",
    "    \n",
    "    # Store the resulting one-hot encoded DataFrame in the airport_data dictionary\n",
    "    airport_data[airport] = y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the predictions from all airports\n",
    "all_predictions = []\n",
    "\n",
    "# Loop over the airports and append the predictions to the all_predictions list\n",
    "for airport in airports:\n",
    "    all_predictions.extend(airport_data[airport])\n",
    "\n",
    "# Convert the all_predictions list to a NumPy array\n",
    "all_predictions_array = np.array(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(submission_format, parse_dates=['timestamp'])\n",
    "\n",
    "results_format = f'data/code_execution_development_data/test_labels.csv'\n",
    "results = pd.read_csv(results_format, parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['minutes_until_pushback'] = all_predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('open_area_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me397_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
